{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sv3d_p import\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from fire import Fire\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from rembg import remove\n",
    "from scripts.util.detection.nsfw_and_watermark_dectection import DeepFloydDataFiltering\n",
    "from sgm.inference.helpers import embed_watermark\n",
    "from sgm.util import default, instantiate_from_config\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_embedder_keys_from_conditioner(conditioner):\n",
    "    return list(set([x.input_key for x in conditioner.embedders]))\n",
    "\n",
    "\n",
    "def get_batch(keys, value_dict, N, T, device):\n",
    "    batch = {}\n",
    "    batch_uc = {}\n",
    "\n",
    "    for key in keys:\n",
    "        if key == \"fps_id\":\n",
    "            batch[key] = (\n",
    "                torch.tensor([value_dict[\"fps_id\"]])\n",
    "                .to(device)\n",
    "                .repeat(int(math.prod(N)))\n",
    "            )\n",
    "        elif key == \"motion_bucket_id\":\n",
    "            batch[key] = (\n",
    "                torch.tensor([value_dict[\"motion_bucket_id\"]])\n",
    "                .to(device)\n",
    "                .repeat(int(math.prod(N)))\n",
    "            )\n",
    "        elif key == \"cond_aug\":\n",
    "            batch[key] = repeat(\n",
    "                torch.tensor([value_dict[\"cond_aug\"]]).to(device),\n",
    "                \"1 -> b\",\n",
    "                b=math.prod(N),\n",
    "            )\n",
    "        elif key == \"cond_frames\" or key == \"cond_frames_without_noise\":\n",
    "            batch[key] = repeat(value_dict[key], \"1 ... -> b ...\", b=N[0])\n",
    "        elif key == \"polars_rad\" or key == \"azimuths_rad\":\n",
    "            batch[key] = torch.tensor(value_dict[key]).to(device).repeat(N[0])\n",
    "        else:\n",
    "            batch[key] = value_dict[key]\n",
    "\n",
    "    if T is not None:\n",
    "        batch[\"num_video_frames\"] = T\n",
    "\n",
    "    for key in batch.keys():\n",
    "        if key not in batch_uc and isinstance(batch[key], torch.Tensor):\n",
    "            batch_uc[key] = torch.clone(batch[key])\n",
    "    return batch, batch_uc\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    config: str,\n",
    "    device: str,\n",
    "    num_frames: int,\n",
    "    num_steps: int,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    config = OmegaConf.load(config)\n",
    "    if device == \"cuda\":\n",
    "        config.model.params.conditioner_config.params.emb_models[\n",
    "            0\n",
    "        ].params.open_clip_embedding_config.params.init_device = device\n",
    "\n",
    "    config.model.params.sampler_config.params.verbose = verbose\n",
    "    config.model.params.sampler_config.params.num_steps = num_steps\n",
    "    config.model.params.sampler_config.params.guider_config.params.num_frames = (\n",
    "        num_frames\n",
    "    )\n",
    "    if device == \"cuda\":\n",
    "        with torch.device(device):\n",
    "            model = instantiate_from_config(config.model).to(device).eval()\n",
    "    else:\n",
    "        model = instantiate_from_config(config.model).to(device).eval()\n",
    "\n",
    "    filter = DeepFloydDataFiltering(verbose=False, device=device)\n",
    "    return model, filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir='/home/luoziqian/Works/Baseline'\n",
    "input_path=os.path.join(root_dir,\"test_model_images/239.png\")\n",
    "num_frames=21\n",
    "model_config=os.path.join(root_dir,'configs/sv3d_p.yaml')\n",
    "num_steps=50\n",
    "elevations_deg=[10]*num_frames\n",
    "polars_rad=[np.deg2rad(90-e) for e in elevations_deg]\n",
    "azimuths_deg=np.linspace(0,360,num_frames+1)[1:]%360\n",
    "azimuths_rad=[np.deg2rad((a-azimuths_deg[-1])%360) for a in azimuths_deg]\n",
    "image_frame_ratio=None\n",
    "motion_bucket_id=127\n",
    "fps_id=6\n",
    "cond_aug=0.02\n",
    "decoding_t=6\n",
    "device='cuda'\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "VideoTransformerBlock is using checkpointing\n",
      "Initialized embedder #0: FrozenOpenCLIPImagePredictionEmbedder with 683800065 params. Trainable: False\n",
      "Initialized embedder #1: VideoPredictionEmbedderWithEncoder with 83653863 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #3: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #4: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Restored from checkpoints/sv3d_p.safetensors with 0 missing and 0 unexpected keys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79020e6df550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,filter=load_model(\n",
    "    model_config,\n",
    "    device,\n",
    "    num_frames,\n",
    "    num_steps,\n",
    "    verbose=False,\n",
    ")\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert input_path is not None\n",
    "assert os.path.exists(os.path.join(root_dir, input_path)) or os.path.exists(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(root_dir, input_path)):\n",
    "    image=Image.open(input_path)\n",
    "elif os.path.exists(input_path):\n",
    "    image=Image.open(os.path.join(root_dir, input_path))\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Could not find file {input_path}\")\n",
    "\n",
    "if image.mode == \"RGBA\":\n",
    "    pass\n",
    "else:\n",
    "    # remove bg\n",
    "    image.thumbnail([768, 768], Image.Resampling.LANCZOS)\n",
    "    image = remove(image.convert(\"RGBA\"), alpha_matting=True)\n",
    "\n",
    "# resize object in frame\n",
    "image_arr = np.array(image)\n",
    "in_w, in_h = image_arr.shape[:2]\n",
    "ret, mask = cv2.threshold(\n",
    "    np.array(image.split()[-1]), 0, 255, cv2.THRESH_BINARY\n",
    ")\n",
    "x, y, w, h = cv2.boundingRect(mask)\n",
    "max_size = max(w, h)\n",
    "side_len = (\n",
    "    int(max_size / image_frame_ratio)\n",
    "    if image_frame_ratio is not None\n",
    "    else in_w\n",
    ")\n",
    "padded_image = np.zeros((side_len, side_len, 4), dtype=np.uint8)\n",
    "center = side_len // 2\n",
    "padded_image[\n",
    "    center - h // 2 : center - h // 2 + h,\n",
    "    center - w // 2 : center - w // 2 + w,\n",
    "] = image_arr[y : y + h, x : x + w]\n",
    "# resize frame to 576x576\n",
    "rgba = Image.fromarray(padded_image).resize((576, 576), Image.LANCZOS)\n",
    "# white bg\n",
    "rgba_arr = np.array(rgba) / 255.0\n",
    "rgb = rgba_arr[..., :3] * rgba_arr[..., -1:] + (1 - rgba_arr[..., -1:])\n",
    "input_image = Image.fromarray((rgb * 255).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = ToTensor()(input_image)\n",
    "image = image * 2.0 - 1.0\n",
    "\n",
    "image = image.unsqueeze(0).to(device)\n",
    "H, W = image.shape[2:]\n",
    "assert image.shape[1] == 3\n",
    "F = 8\n",
    "C = 4\n",
    "shape = (num_frames, C, H // F, W // F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_dict = {}\n",
    "value_dict[\"cond_frames_without_noise\"] = image\n",
    "value_dict[\"motion_bucket_id\"] = motion_bucket_id\n",
    "value_dict[\"fps_id\"] = fps_id\n",
    "value_dict[\"cond_aug\"] = cond_aug\n",
    "value_dict[\"cond_frames\"] = image + cond_aug * torch.randn_like(image)\n",
    "value_dict[\"polars_rad\"] = polars_rad\n",
    "value_dict[\"azimuths_rad\"] = azimuths_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luoziqian/anaconda3/envs/gs_Luo/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.autocast(device):\n",
    "        batch, batch_uc = get_batch(\n",
    "            get_unique_embedder_keys_from_conditioner(model.conditioner),\n",
    "            value_dict,\n",
    "            [1, num_frames],\n",
    "            T=num_frames,\n",
    "            device=device,\n",
    "        )\n",
    "        c, uc = model.conditioner.get_unconditional_conditioning(\n",
    "            batch,\n",
    "            batch_uc=batch_uc,\n",
    "            force_uc_zero_embeddings=[\n",
    "                \"cond_frames\",\n",
    "                \"cond_frames_without_noise\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        for k in [\"crossattn\", \"concat\"]:\n",
    "            uc[k] = repeat(uc[k], \"b ... -> b t ...\", t=num_frames)\n",
    "            uc[k] = rearrange(uc[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
    "            c[k] = repeat(c[k], \"b ... -> b t ...\", t=num_frames)\n",
    "            c[k] = rearrange(c[k], \"b t ... -> (b t) ...\", t=num_frames)\n",
    "\n",
    "        randn = torch.randn(shape, device=device)\n",
    "\n",
    "        additional_model_inputs = {}\n",
    "        additional_model_inputs[\"image_only_indicator\"] = torch.zeros(\n",
    "            2, num_frames\n",
    "        ).to(device)\n",
    "        additional_model_inputs[\"num_video_frames\"] = batch[\"num_video_frames\"]\n",
    "\n",
    "        def denoiser(input, sigma, c):\n",
    "            return model.denoiser(\n",
    "                model.model, input, sigma, c, **additional_model_inputs\n",
    "            )\n",
    "\n",
    "        samples_z = model.sampler(denoiser, randn, cond=c, uc=uc)\n",
    "        model.en_and_decode_n_samples_a_time = decoding_t\n",
    "        samples_x = model.decode_first_stage(samples_z)\n",
    "\n",
    "        samples_x[-1:] = value_dict[\"cond_frames_without_noise\"]\n",
    "        samples = torch.clamp((samples_x + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "        samples = embed_watermark(samples)\n",
    "        samples = filter(samples) # t c h w [21,3,576,576]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luoziqian/Works/Baseline/core/attention.py:22: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n"
     ]
    }
   ],
   "source": [
    "#LGM import\n",
    "import os\n",
    "import tyro\n",
    "import imageio\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from safetensors.torch import load_file\n",
    "import rembg\n",
    "import gradio as gr\n",
    "\n",
    "import kiui\n",
    "from kiui.op import recenter\n",
    "from kiui.cam import orbit_camera\n",
    "\n",
    "from core.options import AllConfigs, Options,config_defaults\n",
    "from core.models import LGM\n",
    "from mvdream.pipeline_mvdream import MVDreamPipeline\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ply_path='pointclouds.ply'\n",
    "output_video_path='video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded checkpoint from pretrained/model_fp16.safetensors\n"
     ]
    }
   ],
   "source": [
    "opt = config_defaults['big']\n",
    "opt.resume='pretrained/model_fp16.safetensors'\n",
    "opt.num_frames=21\n",
    "\n",
    "# model\n",
    "model = LGM(opt)\n",
    "# resume pretrained checkpoint\n",
    "if opt.resume is not None:\n",
    "    if opt.resume.endswith('safetensors'):\n",
    "        ckpt = load_file(opt.resume, device='cpu')\n",
    "    else:\n",
    "        ckpt = torch.load(opt.resume, map_location='cpu')\n",
    "    model.load_state_dict(ckpt, strict=False)\n",
    "    print(f'[INFO] Loaded checkpoint from {opt.resume}')\n",
    "else:\n",
    "    print(f'[WARN] model randomly initialized, are you sure?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.unet.mid_block.attns[0].num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGM(\n",
       "  (unet): UNet(\n",
       "    (conv_in): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): DownBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-1): 2 x None\n",
       "        )\n",
       "        (downsample): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (1): DownBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-1): 2 x None\n",
       "        )\n",
       "        (downsample): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (2): DownBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-1): 2 x None\n",
       "        )\n",
       "        (downsample): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (3): DownBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-1): 2 x MVAttention(\n",
       "            (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (attn): MemEffAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (4): DownBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-1): 2 x MVAttention(\n",
       "            (norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (attn): MemEffAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      )\n",
       "      (5): DownBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Identity()\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-1): 2 x MVAttention(\n",
       "            (norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (attn): MemEffAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): MidBlock(\n",
       "      (nets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock(\n",
       "          (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (shortcut): Identity()\n",
       "        )\n",
       "      )\n",
       "      (attns): ModuleList(\n",
       "        (0): MVAttention(\n",
       "          (norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (up_blocks): ModuleList(\n",
       "      (0): UpBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-2): 3 x MVAttention(\n",
       "            (norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (attn): MemEffAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): UpBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 1536, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1536, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(1536, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-2): 3 x MVAttention(\n",
       "            (norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (attn): MemEffAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): UpBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 1536, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-2): 3 x MVAttention(\n",
       "            (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (attn): MemEffAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (upsample): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (3): UpBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 768, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-2): 3 x None\n",
       "        )\n",
       "        (upsample): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (4): UpBlock(\n",
       "        (nets): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 192, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (shortcut): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attns): ModuleList(\n",
       "          (0-2): 3 x None\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "    (conv_out): Conv2d(128, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv): Conv2d(14, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (lpips_loss): LPIPS(\n",
       "    (net): vgg16(\n",
       "      (slice1): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (slice2): Sequential(\n",
       "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (6): ReLU(inplace=True)\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (8): ReLU(inplace=True)\n",
       "      )\n",
       "      (slice3): Sequential(\n",
       "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (11): ReLU(inplace=True)\n",
       "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (13): ReLU(inplace=True)\n",
       "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (15): ReLU(inplace=True)\n",
       "      )\n",
       "      (slice4): Sequential(\n",
       "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (18): ReLU(inplace=True)\n",
       "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (20): ReLU(inplace=True)\n",
       "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (22): ReLU(inplace=True)\n",
       "      )\n",
       "      (slice5): Sequential(\n",
       "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (25): ReLU(inplace=True)\n",
       "        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (27): ReLU(inplace=True)\n",
       "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (29): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (scaling_layer): ScalingLayer()\n",
       "    (lin0): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lin1): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lin2): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lin3): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lin4): NetLinLayer(\n",
       "      (model): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (lins): ModuleList(\n",
       "      (0): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3-4): 2 x NetLinLayer(\n",
       "        (model): Sequential(\n",
       "          (0): Dropout(p=0.5, inplace=False)\n",
       "          (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.half().to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tan_half_fov = np.tan(0.5 * np.deg2rad(opt.fovy))\n",
    "proj_matrix = torch.zeros(4, 4, dtype=torch.float32, device=device)\n",
    "proj_matrix[0, 0] = 1 / tan_half_fov\n",
    "proj_matrix[1, 1] = 1 / tan_half_fov\n",
    "proj_matrix[2, 2] = (opt.zfar + opt.znear) / (opt.zfar - opt.znear)\n",
    "proj_matrix[3, 2] = - (opt.zfar * opt.znear) / (opt.zfar - opt.znear)\n",
    "proj_matrix[2, 3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = F.interpolate(samples, size=(opt.input_size, opt.input_size), mode='bilinear', align_corners=False)\n",
    "input_image = TF.normalize(input_image, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "rays_embeddings = model.prepare_default_rays(device,num_frames=opt.num_frames,elevation=10)\n",
    "input_image = torch.cat([input_image, rays_embeddings], dim=1).unsqueeze(0) # [1, 4, 9, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        # generate gaussians\n",
    "        gaussians = model.forward_gaussians(input_image)\n",
    "    \n",
    "    # save gaussians\n",
    "    model.gs.save_ply(gaussians, output_ply_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_gaussian_sigle_view:int=int(gaussians.shape[1]/21)\n",
    "num_gaussian_sigle_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 653.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# render 360 video \n",
    "images = []\n",
    "elevation = 0\n",
    "if opt.fancy_video:\n",
    "    azimuth = np.arange(0, 720, 4, dtype=np.int32)\n",
    "    for azi in tqdm.tqdm(azimuth):\n",
    "        \n",
    "        cam_poses = torch.from_numpy(orbit_camera(elevation, azi, radius=opt.cam_radius, opengl=True)).unsqueeze(0).to(device)\n",
    "\n",
    "        cam_poses[:, :3, 1:3] *= -1 # invert up & forward direction\n",
    "        \n",
    "        # cameras needed by gaussian rasterizer\n",
    "        cam_view = torch.inverse(cam_poses).transpose(1, 2) # [V, 4, 4]\n",
    "        cam_view_proj = cam_view @ proj_matrix # [V, 4, 4]\n",
    "        cam_pos = - cam_poses[:, :3, 3] # [V, 3]\n",
    "\n",
    "        scale = min(azi / 360, 1)\n",
    "\n",
    "        image = model.gs.render(gaussians, cam_view.unsqueeze(0), cam_view_proj.unsqueeze(0), cam_pos.unsqueeze(0), scale_modifier=scale)['image']\n",
    "        images.append((image.squeeze(1).permute(0,2,3,1).contiguous().float().cpu().numpy() * 255).astype(np.uint8))\n",
    "else:\n",
    "    azimuth = np.arange(0, 360, 2, dtype=np.int32)\n",
    "    for azi in tqdm.tqdm(azimuth):\n",
    "        \n",
    "        cam_poses = torch.from_numpy(orbit_camera(elevation, azi, radius=opt.cam_radius, opengl=True)).unsqueeze(0).to(device)\n",
    "\n",
    "        cam_poses[:, :3, 1:3] *= -1 # invert up & forward direction\n",
    "        \n",
    "        # cameras needed by gaussian rasterizer\n",
    "        cam_view = torch.inverse(cam_poses).transpose(1, 2) # [V, 4, 4]\n",
    "        cam_view_proj = cam_view @ proj_matrix # [V, 4, 4]\n",
    "        cam_pos = - cam_poses[:, :3, 3] # [V, 3]\n",
    "\n",
    "        image = model.gs.render(gaussians, cam_view.unsqueeze(0), cam_view_proj.unsqueeze(0), cam_pos.unsqueeze(0), scale_modifier=1)['image']\n",
    "        images.append((image.squeeze(1).permute(0,2,3,1).contiguous().float().cpu().numpy() * 255).astype(np.uint8))\n",
    "\n",
    "images = np.concatenate(images, axis=0)\n",
    "imageio.mimwrite(output_video_path, images, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:00<00:00, 700.87it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 722.50it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 725.49it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 714.69it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 722.07it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 737.53it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 733.72it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 730.55it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 708.79it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 759.82it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 724.54it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 734.27it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 719.05it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 730.44it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 737.22it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 719.50it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 754.73it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 748.54it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 763.60it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 764.72it/s]\n",
      "100%|██████████| 180/180 [00:00<00:00, 742.33it/s]\n"
     ]
    }
   ],
   "source": [
    "video_views_dir='video_views'\n",
    "os.makedirs(video_views_dir, exist_ok=True)\n",
    "for i in range(21):\n",
    "    # render 360 video \n",
    "    output_video_path=os.path.join(video_views_dir,f'video_view_{i}.mp4')\n",
    "    images = []\n",
    "    elevation = 0\n",
    "    if opt.fancy_video:\n",
    "        azimuth = np.arange(0, 720, 4, dtype=np.int32)\n",
    "        for azi in tqdm.tqdm(azimuth):\n",
    "            \n",
    "            cam_poses = torch.from_numpy(orbit_camera(elevation, azi, radius=opt.cam_radius, opengl=True)).unsqueeze(0).to(device)\n",
    "\n",
    "            cam_poses[:, :3, 1:3] *= -1 # invert up & forward direction\n",
    "            \n",
    "            # cameras needed by gaussian rasterizer\n",
    "            cam_view = torch.inverse(cam_poses).transpose(1, 2) # [V, 4, 4]\n",
    "            cam_view_proj = cam_view @ proj_matrix # [V, 4, 4]\n",
    "            cam_pos = - cam_poses[:, :3, 3] # [V, 3]\n",
    "\n",
    "            scale = min(azi / 360, 1)\n",
    "\n",
    "            image = model.gs.render(gaussians[:,num_gaussian_sigle_view*i:num_gaussian_sigle_view*(i+1)], cam_view.unsqueeze(0), cam_view_proj.unsqueeze(0), cam_pos.unsqueeze(0), scale_modifier=scale)['image']\n",
    "            images.append((image.squeeze(1).permute(0,2,3,1).contiguous().float().cpu().numpy() * 255).astype(np.uint8))\n",
    "    else:\n",
    "        azimuth = np.arange(0, 360, 2, dtype=np.int32)\n",
    "        for azi in tqdm.tqdm(azimuth):\n",
    "            \n",
    "            cam_poses = torch.from_numpy(orbit_camera(elevation, azi, radius=opt.cam_radius, opengl=True)).unsqueeze(0).to(device)\n",
    "\n",
    "            cam_poses[:, :3, 1:3] *= -1 # invert up & forward direction\n",
    "            \n",
    "            # cameras needed by gaussian rasterizer\n",
    "            cam_view = torch.inverse(cam_poses).transpose(1, 2) # [V, 4, 4]\n",
    "            cam_view_proj = cam_view @ proj_matrix # [V, 4, 4]\n",
    "            cam_pos = - cam_poses[:, :3, 3] # [V, 3]\n",
    "\n",
    "            image = model.gs.render(gaussians[:,num_gaussian_sigle_view*i:num_gaussian_sigle_view*(i+1)], cam_view.unsqueeze(0), cam_view_proj.unsqueeze(0), cam_pos.unsqueeze(0), scale_modifier=1)['image']\n",
    "            images.append((image.squeeze(1).permute(0,2,3,1).contiguous().float().cpu().numpy() * 255).astype(np.uint8))\n",
    "\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    imageio.mimwrite(output_video_path, images, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
